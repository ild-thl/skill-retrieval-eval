{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import Dict, List, Set, Optional, Any, Tuple\n",
    "import json\n",
    "import requests\n",
    "from api_client import APIClient\n",
    "from api_config import APIConfig\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIConfig for evaluating skill prediction APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom formatters for different APIs\n",
    "\n",
    "def competence_analyser_request_formatter(query: str, top_k: int) -> Tuple[Dict, Optional[Dict]]:\n",
    "    \"\"\"\n",
    "    Format request for the competence analyser API v2/chatsearch endpoint\n",
    "    Returns (data, params) tuple\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"doc\": query,  # The course description goes into 'doc' field\n",
    "        \"taxonomies\": [\"ESCO\"],  # Focus on ESCO taxonomy for our evaluation\n",
    "        \"targets\": [\"learning_outcomes\"],  # We want learning outcomes\n",
    "        \"top_k\": top_k,\n",
    "        \"rerank\": True,  # Use reranking\n",
    "        \"finetuned\": True,  # Use fine-tuned models\n",
    "        \"trusted_score\": 0.0,  # Accept all scores, we'll filter later\n",
    "        \"score_cutoff\": 0.0,  # Accept all scores\n",
    "        \"strict\": 0,  # Get all top_k offers, without any cutoff\n",
    "        \"use_llm\": False,  # Don't use LLM extraction to keep it comparable\n",
    "        \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\", \"\"),  # Use environment variable for OpenAI key\n",
    "        \"llm_validation\": False,  # Don't use LLM validation to keep it comparable\n",
    "    }\n",
    "    return data, None  # No URL parameters needed\n",
    "\n",
    "def competence_analyser_response_parser(response) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Parse response from the competence analyser API v2/chatsearch endpoint\n",
    "    Returns list of (skill_name, score) tuples\n",
    "    \"\"\"\n",
    "    result = response.json()\n",
    "    predictions = []\n",
    "    \n",
    "    # The v2 API returns a more complex structure\n",
    "    # We need to extract skills from learning_outcomes -> skills\n",
    "    if \"learning_outcomes\" in result and result[\"learning_outcomes\"]:\n",
    "        learning_outcomes = result[\"learning_outcomes\"]\n",
    "        if \"skills\" in learning_outcomes:\n",
    "            for skill in learning_outcomes[\"skills\"]:\n",
    "                if \"title\" in skill and \"score\" in skill:\n",
    "                    # Note: Higher scores are better in this API\n",
    "                    predictions.append((skill[\"title\"], float(skill[\"score\"])))\n",
    "    \n",
    "    # Sort by score descending (higher is better)\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions\n",
    "\n",
    "def generic_api_request_formatter(query: str, top_k: int) -> Tuple[Dict, Optional[Dict]]:\n",
    "    \"\"\"Generic formatter for simple APIs\"\"\"\n",
    "    data = {\"query\": query, \"top_k\": top_k}\n",
    "    return data, None\n",
    "\n",
    "def generic_api_response_parser(response) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Generic parser for simple API responses\"\"\"\n",
    "    result = response.json()\n",
    "    predictions = []\n",
    "\n",
    "def metadatagen_request_formatter(query: str, top_k: int) -> Tuple[Dict, Optional[Dict]]:\n",
    "    \"\"\"Format request for the MetadataGen API\"\"\"\n",
    "    data = {\n",
    "        \"name\": query,  # Use course name as the query\n",
    "        \"description\": \"This course covers the fundamentals of machine learning including supervised and unsupervised learning algorithms, neural networks, and practical applications in data science.\",\n",
    "        \"top_k\": top_k  # Limit results to top_k skills\n",
    "    }\n",
    "\n",
    "    return data, None  # No URL parameters needed\n",
    "\n",
    "def get_esco_skill_name(skill_uri: str, language: str = 'en', version: str = 'v1.2.0') -> str:\n",
    "    \"\"\"Fetch the ESCO skill name based on the skill ID and language\"\"\"\n",
    "    # https://ec.europa.eu/esco/api/resource/skill?uri=&language=\n",
    "    url = f\"https://ec.europa.eu/esco/api/resource/skill?uri={skill_uri}&language={language}&selectedVersion={version}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result[\"preferredLabel\"][language] if \"preferredLabel\" in result and language in result[\"preferredLabel\"] else null\n",
    "    else:\n",
    "        print(f\"Error fetching skill name for {skill_uri}: {response.status_code} {response.text}\")\n",
    "        return null\n",
    "\n",
    "\n",
    "def metadatagen_response_parser(response) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Parse response from the MetadataGen API\"\"\"\n",
    "    result = response.json()\n",
    "    predictions = []\n",
    "    # The MetadataGen API returns a list of skills with concept URLs and names\n",
    "    if isinstance(result, list):\n",
    "        for index, item in enumerate(result):\n",
    "            if isinstance(item, dict) and \"name\" in item and \"conceptUrl\" in item:\n",
    "                skill_uri = item[\"conceptUrl\"]\n",
    "                # Here we assume descending scores based on index\n",
    "                score = 1.0 - (index / len(result))  # Normalize score to [0, 1]\n",
    "                de_skill_name = get_esco_skill_name(skill_uri, 'de', 'v1.2.0')\n",
    "                predictions.append((de_skill_name, score))\n",
    "            else:\n",
    "                # Handle unexpected item format\n",
    "                print(\"Unexpected item format in response:\", item)\n",
    "                continue\n",
    "    else:\n",
    "        # If the response is not a list, handle it gracefully\n",
    "        print(\"Unexpected response format:\", result)\n",
    "        return []\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)  # Sort by score descending\n",
    "    return predictions\n",
    "    \n",
    "    # Try different common response formats\n",
    "    if \"predictions\" in result:\n",
    "        for item in result[\"predictions\"]:\n",
    "            if isinstance(item, dict) and \"skill\" in item and \"score\" in item:\n",
    "                predictions.append((item[\"skill\"], float(item[\"score\"])))\n",
    "            elif isinstance(item, dict) and \"name\" in item and \"score\" in item:\n",
    "                predictions.append((item[\"name\"], float(item[\"score\"])))\n",
    "    elif \"skills\" in result and \"scores\" in result:\n",
    "        skills = result[\"skills\"]\n",
    "        scores = result[\"scores\"]\n",
    "        predictions = [(skill, float(score)) for skill, score in zip(skills, scores)]\n",
    "    elif isinstance(result, list):\n",
    "        for item in result:\n",
    "            if isinstance(item, dict):\n",
    "                if \"skill\" in item and \"score\" in item:\n",
    "                    predictions.append((item[\"skill\"], float(item[\"score\"])))\n",
    "                elif \"name\" in item and \"score\" in item:\n",
    "                    predictions.append((item[\"name\"], float(item[\"score\"])))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# API Configuration Examples\n",
    "# Customize these configurations based on your actual APIs\n",
    "\n",
    "# Your competence analyser API configuration\n",
    "COMPETENCE_ANALYSER_CONFIG = APIConfig(\n",
    "    name=\"competence_analyser\",\n",
    "    base_url=\"https://lab.dlc.sh/competence-analyser\",\n",
    "    endpoint=\"/v2/chatsearch\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    auth_token=None,  # Add your token if needed\n",
    "    request_format=\"json\",\n",
    "    response_format=\"json\",\n",
    "    max_requests_per_second=2.0,  # Be respectful to the API\n",
    "    timeout=60.0,  # Longer timeout for complex processing\n",
    "    custom_request_formatter=competence_analyser_request_formatter,\n",
    "    custom_response_parser=competence_analyser_response_parser\n",
    ")\n",
    "\n",
    "# MetadataGen API configuration\n",
    "METADATAGEN_API_CONFIG = APIConfig(\n",
    "    name=\"metadataGen\",\n",
    "    base_url=\"http://host.docker.internal\",\n",
    "    endpoint=\"/get_esco_suggestions\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    auth_token=None,  # Add their token if needed\n",
    "    request_format=\"json\",\n",
    "    response_format=\"json\",\n",
    "    max_requests_per_second=2.0,  # Be respectful to external APIs\n",
    "    timeout=45.0,\n",
    "    custom_request_formatter=metadatagen_request_formatter,\n",
    "    custom_response_parser=metadatagen_response_parser\n",
    ")\n",
    "\n",
    "# Additional API configurations can be added here\n",
    "API_CONFIGS = {\n",
    "    \"competence_analyser\": COMPETENCE_ANALYSER_CONFIG,\n",
    "    \"metadata_gen\": METADATAGEN_API_CONFIG,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup evaluation data\n",
    "\n",
    "Only execute one of the following cells to either setup a GRETA or ESCO evalution dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for GRETA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsfile = 'evalGretaModelResults.json'\n",
    "\n",
    "# load greata csv file to pandas dataframe GRETA-Kompetenzmodell_v2.csv\n",
    "greta_pd = pd.read_csv('./data/GRETA/sources/GRETA-Kompetenzmodell_v2.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "# Create new column that combines the columns \"Kompetenzfacetten\", \"Kompetenzaspekte\", \"Kompetenzbereiche\", \"Kompetenzanforderungen\", \"Kompetenzbeschreibung\"\n",
    "# greta_pd['page_content'] = \"Kompetenz: \" + greta_pd['Kompetenzfacette'] + '/n gehört zu /n Kompetenzaspekt: ' + greta_pd['Kompetenzaspekt'] + ', Kompetenzbereich: ' + greta_pd['Kompetenzbereich'] + ',/n Kompetenzanforderungen: ' + greta_pd['Kompetenzanforderungen'] + ', Kompetenzbeschreibung: ' + greta_pd['Kompetenzbeschreibung']\n",
    "greta_pd['page_content'] = greta_pd['Kompetenzfacette'] + ',/nKompetenzanforderungen: ' + greta_pd['Kompetenzanforderungen']\n",
    "# Only get page_content and Kompetenzfacette columns\n",
    "greta_pd = greta_pd[['page_content', 'Kompetenzfacette']]\n",
    "# Rename Kompetenzfacette to title\n",
    "greta_pd = greta_pd.rename(columns={'Kompetenzfacette': 'title'})\n",
    "\n",
    "# Get the evaluation data\n",
    "with open('./data/GRETA/validated_greta_240704.json', 'r', encoding='utf-8') as fIn:\n",
    "    data = json.load(fIn)\n",
    "\n",
    "# Load the data into the DataFrameLoader\n",
    "loader = DataFrameLoader(greta_pd, page_content_column=\"page_content\")\n",
    "documents = loader.load()\n",
    "corpus = {i: d['title'] for i, d in enumerate(greta_pd.to_dict('records'))}\n",
    "queries = {i: d['query'] for i, d in enumerate(data)}\n",
    "\n",
    "relevant_docs = {}\n",
    "for i, d in enumerate(data):\n",
    "    relevant_docs[i] = []\n",
    "    for doc in d['pos']:\n",
    "        for j, c in corpus.items():\n",
    "            if c == doc:\n",
    "                relevant_docs[i].append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for ESCO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsfile = 'evalESCOModelResults.json'\n",
    "\n",
    "# Load texts from json file\n",
    "skills = pd.read_csv(\"./data/ESCO/sources/skills_as_documents_v120.csv\")\n",
    "\n",
    "skills['description'] = skills['description'].fillna('')\n",
    "skills['broaderHierarchyConcepts'] = skills['broaderHierarchyConcepts'].fillna('')\n",
    "skills['broaderSkills'] = skills['broaderSkills'].fillna('')\n",
    "skills['narrowerSkills'] = skills['narrowerSkills'].fillna('')\n",
    "skills['isEssentialForOccupations'] = skills['isEssentialForOccupations'].fillna('')\n",
    "skills['isOptionalForOccupations'] = skills['isOptionalForOccupations'].fillna('')\n",
    "skills['isEssentialForSkills'] = skills['isEssentialForSkills'].fillna('')\n",
    "skills['isOptionalForSkills'] = skills['isOptionalForSkills'].fillna('')\n",
    "\n",
    "# Create a new column that combines preferredLabel and description.\n",
    "skills['page_content'] = skills['preferredLabel'] + \" \\n \" + skills['description']\n",
    "\n",
    "# Add a new column called 'taxonomy' with the value 'ESCO'.\n",
    "skills['taxonomy'] = 'ESCO'\n",
    "\n",
    "# remove row where page_content or title is empty\n",
    "skills = skills[skills['page_content'].notna()]\n",
    "skills = skills[skills['preferredLabel'].notna()]\n",
    "\n",
    "skills = skills[['page_content', 'preferredLabel']]\n",
    "# Are there rows with missing preferredLabel?\n",
    "# Rename Kompetenzfacette to title\n",
    "skills = skills.rename(columns={'preferredLabel': 'title'})\n",
    "\n",
    "# Get the evaluation data\n",
    "import json\n",
    "with open('./data/ESCO/wisy_validated_240704.json', 'r', encoding='utf-8') as fIn:\n",
    "    data = json.load(fIn)\n",
    "\n",
    "# Load  the documents\n",
    "loader = DataFrameLoader(skills, page_content_column=\"page_content\")\n",
    "documents = loader.load()\n",
    "corpus = {i: d['title'] for i, d in enumerate(skills.to_dict('records'))}\n",
    "queries = {i: d['query'] for i, d in enumerate(data)}\n",
    "# relevant_docs = {i: [corpus.index(doc) for doc in d['pos']] for i, d in enumerate(data)}\n",
    "relevant_docs = {}\n",
    "for i, d in enumerate(data):\n",
    "    relevant_docs[i] = []\n",
    "    for doc in d['pos']:\n",
    "        for j, c in corpus.items():\n",
    "            if c == doc:\n",
    "                relevant_docs[i].append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_evaluator import RetrievalEvaluator\n",
    "evaluator = RetrievalEvaluator(queries, corpus, relevant_docs, store_docs=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelresults = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelresults[\"isy-finetuned\"] = evaluator(\"isy-thl/multilingual-e5-base-course-skill-tuned\", use_cached_db=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DB = True # Use same vectorstore from previous run, because the embedding model did not change\n",
    "modelresults[\"isy-finetuned-w-reranker\"] = evaluator(\"isy-thl/multilingual-e5-base-course-skill-tuned\", reranker_model_name=\"isy-thl/bge-reranker-base-course-skill-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DB = False # Build new vectorstore\n",
    "modelresults[\"all-MiniLM-L6-v2\"] = evaluator(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelresults[\"bge_base\"] = evaluator(\"BAAI/bge-base-en-v1.5\")\n",
    "# modelresults[\"bge_finetuned\"] = evaluator(\"bge_finetuned_no_sync\")\n",
    "# modelresults[\"bge_greta_finetuned\"] = evaluator(\"bge_greta_finetuned_no_sync\")\n",
    "# modelresults[\"bge_m3\"] = evaluator(\"BAAI/bge-m3\")\n",
    "# modelresults[\"bge_m3_greta_finetuned\"] = evaluator(\"bge_m3_greta_finetuned_no_sync\")\n",
    "# modelresults[\"bge_m3_finetuned\"] = evaluator(\"bge_m3_finetuned_no_sync\")\n",
    "# modelresults[\"snowflake-arctic-embed-l\"] = evaluator(\"Snowflake/snowflake-arctic-embed-l\")\n",
    "# modelresults[\"multilingual-e5-base\"] = evaluator(\"intfloat/multilingual-e5-base\", query_instruction=\"query: \", embed_instruction=\"passage: \")\n",
    "# modelresults[\"multilingual_e5_greta_finetuned\"] = evaluator(\"multilingual_e5_greta_finetuned_no_sync\", query_instruction=\"query: \", embed_instruction=\"passage: \")\n",
    "# modelresults[\"multilingual_e5_finetuned\"] = evaluator(\"multilingual_e5_finetuned_no_sync\", query_instruction=\"query: \", embed_instruction=\"passage: \")\n",
    "# modelresults[\"multilingual_e5_m3_finetuned\"] = evaluator(\"multilingual_e5_m3_finetuned_no_sync\", query_instruction=\"query: \", embed_instruction=\"passage: \")\n",
    "# modelresults[\"multilingual_finetuned_esco6000\"] = evaluator(\"multilingual_finetuned_esco6000_no_sync\", query_instruction=\"query: \", embed_instruction=\"passage: \")\n",
    "# modelresults[\"multilingual_finetuned_esco1500\"] = evaluator(\"multilingual_finetuned_esco1500_no_sync\", query_instruction=\"query: \", embed_instruction=\"passage: \")\n",
    "# modelresults[\"mixed_multilingual_finetuned\"] = evaluator(\"mixed_multilingual_finetuned_no_sync\", query_instruction=\"query: \", embed_instruction=\"passage: \")\n",
    "# CACHE_DB = False\n",
    "# modelresults[\"bge_reranker_finetuned\"] = evaluator(\"multilingual_e5_finetuned_no_sync\", reranker_model_name=\"bge_reranker_finetuned_no_sync\")\n",
    "# CACHE_DB = True\n",
    "# modelresults[\"bge_reranker_greta_finetuned\"] = evaluator(\"multilingual_e5_finetuned_no_sync\", reranker_model_name=\"bge_reranker_greta_finetuned_no_sync\")\n",
    "# CACHE_DB = True\n",
    "# modelresults[\"bge_reranker_skillfit\"] = evaluator(\"multilingual_e5_finetuned_no_sync\", reranker_model_name=\"pascalhuerten/bge_reranker_skillfit\")\n",
    "# CACHE_DB = False\n",
    "# modelresults[\"instructor-base\"] = evaluator(\"hkunlp/instructor-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Testing Section\n",
    "\n",
    "Now we can test APIs alongside the embedding models. Configure your API endpoints above and run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API connectivity before running full evaluation\n",
    "def test_api_connectivity(api_config: APIConfig, test_query: str = \"Python programming\"):\n",
    "    \"\"\"Test if an API is accessible and returns valid responses\"\"\"\n",
    "    try:\n",
    "        client = APIClient(api_config)\n",
    "        predictions = client.predict(test_query, top_k=5)\n",
    "        \n",
    "        print(f\"✅ API '{api_config.name}' is accessible\")\n",
    "        print(f\"   Base URL: {api_config.base_url}\")\n",
    "        print(f\"   Test query: '{test_query}'\")\n",
    "        print(f\"   Returned {len(predictions)} predictions:\")\n",
    "        \n",
    "        for i, (skill, score) in enumerate(predictions[:3], 1):\n",
    "            print(f\"   {i}. {skill} (score: {score:.4f})\")\n",
    "        \n",
    "        if len(predictions) > 3:\n",
    "            print(f\"   ... and {len(predictions) - 3} more\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ API '{api_config.name}' failed connectivity test:\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Test connectivity for all configured APIs\n",
    "print(\"Testing API connectivity...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "api_test_results = {}\n",
    "for api_name, api_config in API_CONFIGS.items():\n",
    "    print(f\"\\nTesting {api_name}:\")\n",
    "    api_test_results[api_name] = test_api_connectivity(api_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"API Connectivity Summary:\")\n",
    "for api_name, is_working in api_test_results.items():\n",
    "    status = \"✅ Working\" if is_working else \"❌ Failed\"\n",
    "    print(f\"  {api_name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run API evaluations\n",
    "# Only run evaluations for APIs that passed connectivity tests\n",
    "for api_name, api_config in API_CONFIGS.items():\n",
    "    if api_name in modelresults:\n",
    "        print(f\"Skipping {api_name} - already evaluated\")\n",
    "        continue\n",
    "    print(f\"\\n🚀 Starting evaluation for {api_name}...\")\n",
    "    try:\n",
    "        modelresults[api_name] = evaluator(api_config=api_config)\n",
    "        print(f\"✅ Completed evaluation for {api_name}\")\n",
    "    except Exception as e:\n",
    "        # Print error and stack trace\n",
    "        print(f\"❌ Evaluation failed for {api_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        modelresults[api_name] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results to a file or merge with existing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Read the existing data\n",
    "existingresults = {}\n",
    "if os.path.exists(resultsfile):\n",
    "    with open(resultsfile, 'r', encoding='utf-8') as fIn:\n",
    "        existingresults = json.load(fIn)\n",
    "# Merge the two dictionaries\n",
    "for model, results in modelresults.items():\n",
    "    existingresults[model] = results\n",
    "\n",
    "# Write the new dictionary back to the file\n",
    "with open(resultsfile, 'w', encoding='utf-8') as fOut:\n",
    "    json.dump(existingresults, fOut, indent=4)\n",
    "\n",
    "modelresults = existingresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the results as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results as a table\n",
    "import pandas as pd\n",
    "\n",
    "# compare results as a table\n",
    "import pandas as pd\n",
    "\n",
    "def get_result_df(modelresults):\n",
    "    results = pd.DataFrame({\n",
    "        'Model': list(modelresults.keys()),\n",
    "        'accuracy@1': [modelresults[model]['accuracy@1'] for model in modelresults],\n",
    "        'accuracy@3': [modelresults[model]['accuracy@3'] for model in modelresults],\n",
    "        'accuracy@5': [modelresults[model]['accuracy@5'] for model in modelresults],\n",
    "        'accuracy@10': [modelresults[model]['accuracy@10'] for model in modelresults],\n",
    "        'precision@1': [modelresults[model]['precision@1'] for model in modelresults],\n",
    "        'precision@3': [modelresults[model]['precision@3'] for model in modelresults],\n",
    "        'precision@5': [modelresults[model]['precision@5'] for model in modelresults],\n",
    "        'precision@10': [modelresults[model]['precision@10'] for model in modelresults],\n",
    "        'recall@1': [modelresults[model]['recall@1'] for model in modelresults],\n",
    "        'recall@3': [modelresults[model]['recall@3'] for model in modelresults],\n",
    "        'recall@5': [modelresults[model]['recall@5'] for model in modelresults],\n",
    "        'recall@10': [modelresults[model]['recall@10'] for model in modelresults],\n",
    "        'ndcg@10': [modelresults[model]['ndcg@10'] for model in modelresults],\n",
    "        'mrr@10': [modelresults[model]['mrr@10'] for model in modelresults],\n",
    "        'map@100': [modelresults[model]['map@100'] for model in modelresults],\n",
    "        # 'avg_time_per_1000_chars': [modelresults[model]['avg_time_per_1000_chars'] for model in modelresults],\n",
    "        'avg_time_per_query': [modelresults[model]['avg_time_per_query'] for model in modelresults],\n",
    "        # 'total_time': [modelresults[model]['total_time'] for model in modelresults]\n",
    "    })\n",
    "    return results\n",
    "\n",
    "# Filter modelresults for these modelnames in that order\n",
    "filtered_results = modelresults\n",
    "# filter_models = ['instructor-base', 'instructor-large', 'instructor-skillfit', 'bge_base', 'bge_greta_finetuned', 'bge_finetuned', 'bge_m3', 'bge_m3_greta_finetuned', 'bge_m3_finetuned', 'multilingual-e5-base', 'multilingual_e5_greta_finetuned', 'multilingual_e5_finetuned', 'mle5f+bge_reranker_skillfit', 'mle5f+bge_reranker_greta_finetuned', 'mle5f+bge_reranker_finetuned']\n",
    "# filter_models = ['intfloat/multilingual-e5-base', 'isy-thl/multilingual-e5-base-course-skill-tuned', 'isy-thl/bge-reranker-base-course-skill-tuned']\n",
    "# filtered_results = {model: modelresults[model] for model in filter_models}\n",
    "results = get_result_df(filtered_results)\n",
    "\n",
    "# filter_models = ['instructor-base', 'instructor-large', 'instructor-skillfit', 'bge_base', 'bge_greta_finetuned', 'bge_finetuned', 'bge_m3', 'bge_m3_greta_finetuned', 'bge_m3_finetuned', 'multilingual-e5-base', 'multilingual_e5_greta_finetuned', 'multilingual_e5_finetuned', 'bge_reranker_skillfit', 'bge_reranker_greta_finetuned', 'bge_reranker_finetuned']\n",
    "\n",
    "\n",
    "# def highlight_max(s):\n",
    "#     '''\n",
    "#     Highlight the maximum in a Series yellow.\n",
    "#     '''\n",
    "#     is_max = s == s.max()\n",
    "#     return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "def grade_by_rank(s):\n",
    "    # skip if column Model\n",
    "    if s.name == 'Model':\n",
    "        return ['' for v in s]\n",
    "    # Get count of values\n",
    "    count = len(s)\n",
    "    reverse = False\n",
    "    if 'time' in s.name:\n",
    "        reverse = True\n",
    "    ordered = s.sort_values(ascending=reverse)\n",
    "    # Define a lighter green RGB\n",
    "    good = (120, 225, 60)\n",
    "    # Define a grey RGB\n",
    "    bad = (230, 230, 230)\n",
    "    colors = []\n",
    "    if count == 1:\n",
    "        # If there is only one value, color it grey\n",
    "        colors.append('background-color: rgb(230,230,230)')\n",
    "    else:\n",
    "        for i, v in enumerate(ordered):\n",
    "            # Linear interpolation (lerp) between red and light green\n",
    "            r = int(good[0] + (bad[0] - good[0]) * (i / (count - 1)))\n",
    "            g = int(good[1] + (bad[1] - good[1]) * (i / (count - 1)))\n",
    "            b = int(good[2] + (bad[2] - good[2]) * (i / (count - 1)))\n",
    "            colors.append(f'background-color: rgb({r},{g},{b})')\n",
    "    \n",
    "    # Make best color even more vibrant\n",
    "    colors[0] = 'background-color: rgb(110, 235, 55)'\n",
    "    \n",
    "    # Assign colors to the original values based on their rank\n",
    "    styles = [colors[ordered.index.get_loc(i)] for i in s.index]\n",
    "    return styles\n",
    "\n",
    "\n",
    "# Apply the function along the DataFrame's columns\n",
    "styled_results = results.style.apply(grade_by_rank, axis=0)\n",
    "        \n",
    "styled_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
